{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a133de-6004-4a9d-9fba-84928c39f1f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n+-------+----------+------+\n|   Name|Department|Salary|\n+-------+----------+------+\n|  James|     Sales|  3000|\n|Michael|     Sales|  4600|\n| Robert|     Sales|  4100|\n|  Maria|   Finance|  3000|\n|  James|     Sales|  3000|\n|  Scott|   Finance|  3300|\n|    Jen|   Finance|  3900|\n|   Jeff| Marketing|  3000|\n|  Kumar| Marketing|  2000|\n|   Saif|     Sales|  4100|\n+-------+----------+------+\n\nDataFrame after removing duplicates:\n+-------+----------+------+\n|   Name|Department|Salary|\n+-------+----------+------+\n|Michael|     Sales|  4600|\n|  James|     Sales|  3000|\n| Robert|     Sales|  4100|\n|  Maria|   Finance|  3000|\n|  Scott|   Finance|  3300|\n|    Jen|   Finance|  3900|\n|  Kumar| Marketing|  2000|\n|   Jeff| Marketing|  3000|\n|   Saif|     Sales|  4100|\n+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1. Find the distinct columns and remove duplicate from the following dataset\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\"\"\"\"\n",
    "  \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DistinctColumns\").getOrCreate()\n",
    "\n",
    "# Define the dataset\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "        (\"Michael\", \"Sales\", 4600),\n",
    "        (\"Robert\", \"Sales\", 4100),\n",
    "        (\"Maria\", \"Finance\", 3000),\n",
    "        (\"James\", \"Sales\", 3000),\n",
    "        (\"Scott\", \"Finance\", 3300),\n",
    "        (\"Jen\", \"Finance\", 3900),\n",
    "        (\"Jeff\", \"Marketing\", 3000),\n",
    "        (\"Kumar\", \"Marketing\", 2000),\n",
    "        (\"Saif\", \"Sales\", 4100)]\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Find distinct columns and remove duplicates\n",
    "distinct_df = df.dropDuplicates()\n",
    "\n",
    "# Display the DataFrame after removing duplicates\n",
    "print(\"DataFrame after removing duplicates:\")\n",
    "distinct_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a64d97-7a2a-4d0b-bf75-81213d1bef6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp DataFrame:\n+------+--------+-----------+--------+---------------+------+------+\n|emp_id|emp_name|emp_dept_id|emp_year|emp_dept_id_ref|gender|salary|\n+------+--------+-----------+--------+---------------+------+------+\n|     1|   Smith|         -1|    2018|             10|     M|  3000|\n|     2|    Rose|          1|    2010|             20|     M|  4000|\n|     3|Williams|          1|    2010|             10|     M|  1000|\n|     4|   Jones|          2|    2005|             10|     F|  2000|\n|     5|   Brown|          2|    2010|             40|      |    -1|\n|     6|   Brown|          2|    2010|             50|      |    -1|\n+------+--------+-----------+--------+---------------+------+------+\n\ndept DataFrame:\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\nInner Join:\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|emp_id|emp_name|emp_dept_id|emp_year|emp_dept_id_ref|gender|salary|dept_name|dept_id|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|     1|   Smith|         -1|    2018|             10|     M|  3000|  Finance|     10|\n|     3|Williams|          1|    2010|             10|     M|  1000|  Finance|     10|\n|     4|   Jones|          2|    2005|             10|     F|  2000|  Finance|     10|\n|     2|    Rose|          1|    2010|             20|     M|  4000|Marketing|     20|\n|     5|   Brown|          2|    2010|             40|      |    -1|       IT|     40|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n\nOuter Join:\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|emp_id|emp_name|emp_dept_id|emp_year|emp_dept_id_ref|gender|salary|dept_name|dept_id|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|     1|   Smith|         -1|    2018|             10|     M|  3000|  Finance|     10|\n|     3|Williams|          1|    2010|             10|     M|  1000|  Finance|     10|\n|     4|   Jones|          2|    2005|             10|     F|  2000|  Finance|     10|\n|     2|    Rose|          1|    2010|             20|     M|  4000|Marketing|     20|\n|  null|    null|       null|    null|           null|  null|  null|    Sales|     30|\n|     5|   Brown|          2|    2010|             40|      |    -1|       IT|     40|\n|     6|   Brown|          2|    2010|             50|      |    -1|     null|   null|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n\nLeft Join:\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|emp_id|emp_name|emp_dept_id|emp_year|emp_dept_id_ref|gender|salary|dept_name|dept_id|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|     1|   Smith|         -1|    2018|             10|     M|  3000|  Finance|     10|\n|     3|Williams|          1|    2010|             10|     M|  1000|  Finance|     10|\n|     2|    Rose|          1|    2010|             20|     M|  4000|Marketing|     20|\n|     4|   Jones|          2|    2005|             10|     F|  2000|  Finance|     10|\n|     6|   Brown|          2|    2010|             50|      |    -1|     null|   null|\n|     5|   Brown|          2|    2010|             40|      |    -1|       IT|     40|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n\nRight Join:\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|emp_id|emp_name|emp_dept_id|emp_year|emp_dept_id_ref|gender|salary|dept_name|dept_id|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|     4|   Jones|          2|    2005|             10|     F|  2000|  Finance|     10|\n|     3|Williams|          1|    2010|             10|     M|  1000|  Finance|     10|\n|     1|   Smith|         -1|    2018|             10|     M|  3000|  Finance|     10|\n|     2|    Rose|          1|    2010|             20|     M|  4000|Marketing|     20|\n|  null|    null|       null|    null|           null|  null|  null|    Sales|     30|\n|     5|   Brown|          2|    2010|             40|      |    -1|       IT|     40|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n\nCross Join:\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|emp_id|emp_name|emp_dept_id|emp_year|emp_dept_id_ref|gender|salary|dept_name|dept_id|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\n|     1|   Smith|         -1|    2018|             10|     M|  3000|  Finance|     10|\n|     1|   Smith|         -1|    2018|             10|     M|  3000|Marketing|     20|\n|     1|   Smith|         -1|    2018|             10|     M|  3000|    Sales|     30|\n|     1|   Smith|         -1|    2018|             10|     M|  3000|       IT|     40|\n|     2|    Rose|          1|    2010|             20|     M|  4000|  Finance|     10|\n|     3|Williams|          1|    2010|             10|     M|  1000|  Finance|     10|\n|     2|    Rose|          1|    2010|             20|     M|  4000|Marketing|     20|\n|     3|Williams|          1|    2010|             10|     M|  1000|Marketing|     20|\n|     2|    Rose|          1|    2010|             20|     M|  4000|    Sales|     30|\n|     3|Williams|          1|    2010|             10|     M|  1000|    Sales|     30|\n|     2|    Rose|          1|    2010|             20|     M|  4000|       IT|     40|\n|     3|Williams|          1|    2010|             10|     M|  1000|       IT|     40|\n|     4|   Jones|          2|    2005|             10|     F|  2000|  Finance|     10|\n|     4|   Jones|          2|    2005|             10|     F|  2000|Marketing|     20|\n|     4|   Jones|          2|    2005|             10|     F|  2000|    Sales|     30|\n|     4|   Jones|          2|    2005|             10|     F|  2000|       IT|     40|\n|     5|   Brown|          2|    2010|             40|      |    -1|  Finance|     10|\n|     6|   Brown|          2|    2010|             50|      |    -1|  Finance|     10|\n|     5|   Brown|          2|    2010|             40|      |    -1|Marketing|     20|\n|     6|   Brown|          2|    2010|             50|      |    -1|Marketing|     20|\n+------+--------+-----------+--------+---------------+------+------+---------+-------+\nonly showing top 20 rows\n\nAnti Join:\n+------+--------+-----------+--------+---------------+------+------+\n|emp_id|emp_name|emp_dept_id|emp_year|emp_dept_id_ref|gender|salary|\n+------+--------+-----------+--------+---------------+------+------+\n|     6|   Brown|          2|    2010|             50|      |    -1|\n+------+--------+-----------+--------+---------------+------+------+\n\nSemi Join:\n+------+--------+-----------+--------+---------------+------+------+\n|emp_id|emp_name|emp_dept_id|emp_year|emp_dept_id_ref|gender|salary|\n+------+--------+-----------+--------+---------------+------+------+\n|     1|   Smith|         -1|    2018|             10|     M|  3000|\n|     3|Williams|          1|    2010|             10|     M|  1000|\n|     4|   Jones|          2|    2005|             10|     F|  2000|\n|     2|    Rose|          1|    2010|             20|     M|  4000|\n|     5|   Brown|          2|    2010|             40|      |    -1|\n+------+--------+-----------+--------+---------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2. Let’s create an \"emp\" and \"dept\" DataFrames. here, column \"emp_id\" is unique \n",
    "on emp and \"dept_id\" is unique on the dept dataset’s and emp_dept_id from emp has a reference to dept_id on dept dataset.\"\"\"\n",
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "  \n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "  \n",
    " joins two datasets on key columns give the results of all the following joins and print the results\n",
    " \n",
    "inner\n",
    "outer, full, fullouter, full_outer\n",
    "left, leftouter, left_outer\n",
    "right, rightouter, right_outer\n",
    "cross\t\n",
    "anti, leftanti, left_anti\t\n",
    "semi, leftsemi, left_semi\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"JoinExamples\").getOrCreate()\n",
    "\n",
    "# Define the \"emp\" dataset\n",
    "emp_data = [(1, \"Smith\", -1, \"2018\", \"10\", \"M\", 3000),\n",
    "            (2, \"Rose\", 1, \"2010\", \"20\", \"M\", 4000),\n",
    "            (3, \"Williams\", 1, \"2010\", \"10\", \"M\", 1000),\n",
    "            (4, \"Jones\", 2, \"2005\", \"10\", \"F\", 2000),\n",
    "            (5, \"Brown\", 2, \"2010\", \"40\", \"\", -1),\n",
    "            (6, \"Brown\", 2, \"2010\", \"50\", \"\", -1)]\n",
    "\n",
    "# Define the \"dept\" dataset\n",
    "dept_data = [(\"Finance\", 10),\n",
    "             (\"Marketing\", 20),\n",
    "             (\"Sales\", 30),\n",
    "             (\"IT\", 40)]\n",
    "\n",
    "# Create DataFrames from the datasets\n",
    "emp_columns = [\"emp_id\", \"emp_name\", \"emp_dept_id\", \"emp_year\", \"emp_dept_id_ref\", \"gender\", \"salary\"]\n",
    "dept_columns = [\"dept_name\", \"dept_id\"]\n",
    "\n",
    "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
    "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
    "\n",
    "# Display the original DataFrames\n",
    "print(\"emp DataFrame:\")\n",
    "emp_df.show()\n",
    "\n",
    "print(\"dept DataFrame:\")\n",
    "dept_df.show()\n",
    "\n",
    "# Perform different types of joins and display the results\n",
    "# Inner Join\n",
    "inner_join = emp_df.join(dept_df, emp_df[\"emp_dept_id_ref\"] == dept_df[\"dept_id\"], \"inner\")\n",
    "print(\"Inner Join:\")\n",
    "inner_join.show()\n",
    "\n",
    "# Outer Join (Full Outer Join)\n",
    "outer_join = emp_df.join(dept_df, emp_df[\"emp_dept_id_ref\"] == dept_df[\"dept_id\"], \"outer\")\n",
    "print(\"Outer Join:\")\n",
    "outer_join.show()\n",
    "\n",
    "# Left Join (Left Outer Join)\n",
    "left_join = emp_df.join(dept_df, emp_df[\"emp_dept_id_ref\"] == dept_df[\"dept_id\"], \"left\")\n",
    "print(\"Left Join:\")\n",
    "left_join.show()\n",
    "\n",
    "# Right Join (Right Outer Join)\n",
    "right_join = emp_df.join(dept_df, emp_df[\"emp_dept_id_ref\"] == dept_df[\"dept_id\"], \"right\")\n",
    "print(\"Right Join:\")\n",
    "right_join.show()\n",
    "\n",
    "# Cross Join\n",
    "cross_join = emp_df.crossJoin(dept_df)\n",
    "print(\"Cross Join:\")\n",
    "cross_join.show()\n",
    "\n",
    "# Anti Join (Left Anti Join)\n",
    "anti_join = emp_df.join(dept_df, emp_df[\"emp_dept_id_ref\"] == dept_df[\"dept_id\"], \"left_anti\")\n",
    "print(\"Anti Join:\")\n",
    "anti_join.show()\n",
    "\n",
    "# Semi Join (Left Semi Join)\n",
    "semi_join = emp_df.join(dept_df, emp_df[\"emp_dept_id_ref\"] == dept_df[\"dept_id\"], \"left_semi\")\n",
    "print(\"Semi Join:\")\n",
    "semi_join.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a316c8e-702d-4d56-bd0f-397e8c3b6746",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------------+----+----+----------+----+------------+----------+-----+\n|   Name|Department|Salary|         cume_dist| lag|lead|row_number|rank|percent_rank|dense_rank|ntile|\n+-------+----------+------+------------------+----+----+----------+----+------------+----------+-----+\n|  Maria|   Finance|  3000|0.3333333333333333|null|3300|         1|   1|         0.0|         1|    1|\n|  Scott|   Finance|  3300|0.6666666666666666|3000|3900|         2|   2|         0.5|         2|    2|\n|    Jen|   Finance|  3900|               1.0|3300|null|         3|   3|         1.0|         3|    3|\n|  Kumar| Marketing|  2000|               0.5|null|3000|         1|   1|         0.0|         1|    1|\n|   Jeff| Marketing|  3000|               1.0|2000|null|         2|   2|         1.0|         2|    2|\n|  James|     Sales|  3000|               0.4|null|3000|         1|   1|         0.0|         1|    1|\n|  James|     Sales|  3000|               0.4|3000|4100|         2|   1|         0.0|         1|    1|\n| Robert|     Sales|  4100|               0.8|3000|4100|         3|   3|         0.5|         2|    2|\n|   Saif|     Sales|  4100|               0.8|4100|4600|         4|   3|         0.5|         2|    2|\n|Michael|     Sales|  4600|               1.0|4100|null|         5|   5|         1.0|         3|    3|\n+-------+----------+------+------------------+----+----+----------+----+------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"3.Take the following sample data and apply the window functions cume_dist(), lag, lead, row_number(), rank(), percent_rank(), dense_rank() and ntile\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import cume_dist, lag, lead, row_number, rank, percent_rank, dense_rank, ntile\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WindowFunctionsExample\").getOrCreate()\n",
    "\n",
    "# Define the \"simpleData\" dataset\n",
    "simple_data = [\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(simple_data, columns)\n",
    "\n",
    "# Define a window specification based on the \"Department\" column\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(\"Salary\")\n",
    "\n",
    "# Apply window functions\n",
    "df = df.withColumn(\"cume_dist\", cume_dist().over(window_spec))\n",
    "df = df.withColumn(\"lag\", lag(\"Salary\").over(window_spec))\n",
    "df = df.withColumn(\"lead\", lead(\"Salary\").over(window_spec))\n",
    "df = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "df = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "df = df.withColumn(\"percent_rank\", percent_rank().over(window_spec))\n",
    "df = df.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "df = df.withColumn(\"ntile\", ntile(3).over(window_spec))\n",
    "\n",
    "# Display the result\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb1d7d3f-5dc9-45c7-abec-c174db24cbc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m\"<command-2330113905308507>\"\u001B[0;36m, line \u001B[0;32m16\u001B[0m\n",
       "\u001B[0;31m    df = spark.read.csv(\"C:\\Users\\2000108495\\Downloads\\zipcodes.csv\", header=True, schema=schema)\u001B[0m\n",
       "\u001B[0m                                                                    ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m\"<command-2330113905308507>\"\u001B[0;36m, line \u001B[0;32m16\u001B[0m\n\u001B[0;31m    df = spark.read.csv(\"C:\\Users\\2000108495\\Downloads\\zipcodes.csv\", header=True, schema=schema)\u001B[0m\n\u001B[0m                                                                    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<command-2330113905308507>, line 16)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ReplaceNA\").getOrCreate()\n",
    "\n",
    "# Define the schema using StructType\n",
    "schema = StructType([\n",
    "    StructField(\"ZipCode\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV file with the defined schema\n",
    "df = spark.read.csv(\"C:\\Users\\2000108495\\Downloads\\zipcodes.csv\", header=True, schema=schema)\n",
    "\n",
    "# Replace NA values with NULL\n",
    "df = df.na.fill(\"NULL\")\n",
    "\n",
    "# Save the output as a CSV file\n",
    "df.write.csv(\"path/to/output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22072528-7584-40fc-bc62-77a2240751a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "\u001B[0;32m<command-2330113905308508>\u001B[0m in \u001B[0;36m<cell line: 16>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[0;31m# Read the CSV file with the defined schema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 16\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"zipcodes.csv\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     18\u001B[0m \u001B[0;31m# Replace NA values with NULL\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n",
       "\u001B[1;32m    533\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    534\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 535\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    536\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    537\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: zipcodes.csv"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-2330113905308508>\u001B[0m in \u001B[0;36m<cell line: 16>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;31m# Read the CSV file with the defined schema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"zipcodes.csv\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;31m# Replace NA values with NULL\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    533\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 535\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    536\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: zipcodes.csv",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: Path must be absolute: zipcodes.csv",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ReplaceNA\").getOrCreate()\n",
    "\n",
    "# Define the schema using StructType\n",
    "schema = StructType([\n",
    "    StructField(\"ZipCode\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV file with the defined schema\n",
    "df = spark.read.csv(\"zipcodes.csv\", header=True, schema=schema)\n",
    "\n",
    "# Replace NA values with NULL\n",
    "df = df.na.fill(\"NULL\")\n",
    "\n",
    "# Save the output as a CSV file\n",
    "df.write.csv(\"output_folder\", header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8d51c63-aeac-43b0-a42c-8a7bff7bec73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_casestudies",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
